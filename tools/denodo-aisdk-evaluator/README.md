# Denodo AI SDK Evaluator

<div style="text-align:center">
<img src="static/den.png"> 
</div>

## Introduction

**Denodo AI SDK Evaluator** is a specialized tool for assessing the performance and accuracy of VQL queries generated using the Denodo AI SDK. This tool provides quick feedback on the effectiveness of AI integrations with the Denodo AI SDK, facilitating improvements in accuracy and performance through prompt and context enhancement. It is designed to work with Denodo's AI SDK.

## Features

- **Result Set Correctness:** Executes each generated VQL query and compares its results against a ground truth VQL query using F1 score metrics. This handles uneven result sets through precision/recall calculation, ensuring accurate comparison even when dimensions differ.
- **Result Set Overlap Percentage:** Measures the percentage of overlapping values between generated and reference result sets, providing an intuitive metric for how closely the outputs match.
- **Efficiency Evaluation (reward VES):** Measures execution duration across multiple iterations to determine query efficiency against the ground truth. The Valid Efficiency Score (VES) indicates how efficient the predicted query runs compared to the ground-truth query where the max is 2. If the queries returned different result sets, the result is automatically 0 and the VES calculation is not done.
- **Difficulty-Based Analysis:** Groups and analyzes all metrics by difficulty levels (e.g., simple, moderate, challenging) to provide insights into query performance across complexity levels.

## Generated Excel Output

The evaluator produces a comprehensive Excel file with two sheets; Summary and Details:

### Summary Sheet

This sheet offers an aggregated overview of the assessments, typically grouped by difficulty level. It displays key metrics such as:

- **Percent Subset:** Average percentage across all questions of matching values between rows of matching index.
- **Percent Correct Queries:** The percentage of generated queries whose result sets completely match the ground truth.
- **Number of Samples:** Count of queries in each group and total.
- **Mean Time (s):** Average total execution time from the AI SDK.
- **Time Std Dev:** Standard deviation of AI SDK execution times.
Charts are embedded with excel native charts to visually represent these metrics and to allow further post procesing from the excel UI.

### Details Sheet

This sheet contains the individual evaluation results for each question/query pair, including:

- **Question ID:** Unique identifier for each question.
- **Difficulty:** Query difficulty level.
- **VQL Generated:** The VQL query generated by the AI model.
- **Ground Truth VQL:** The ground truth VQL query.
- **Results Match:** 1 if generated query results exactly match ground truth results, 0 otherwise (derived from VES evaluation).
- **Subsetting Percentage:** The percentage of generated results that are found in the ground truth. Order of rows matters, but order of columns within a row does not for element matching.
- **Percent Overlap:** Percentage of expected elements from the ground truth set that are included in the generated result set. Order of rows and columns matters for this value-based comparison.
- **Have Same Row Count:** 1 if the number of rows match between generated and ground truth query results, 0 otherwise.
- **Bird Standard F1:** F1 score calculated based on the BIRD benchmark methodology, considering cell-wise matches.
- **VES Score:** Valid Efficiency Score, indicating query execution efficiency relative to the ground truth.
- **AI SDK Timing Metrics:** `sql_execution_time`, `vector_store_search_time`, `llm_time`, `total_execution_time`.

## Understanding The Results

Key metrics to help you quickly assess query performance:

- **Percent Correct Queries (Summary):** Shows the percentage of queries that produce exactly matching result sets compared to ground truth. This is a strict measure.
- **Percent Subset (Summary & Details):** Indicates how well the content of generated results aligns with ground truth results on a row-by-row basis. Higher values suggest generated queries retrieve relevant data accurately.
- **Percent Overlap (Details):** Measures the overall commonality of values between the entire generated and ground truth result sets.
- **VES Score (Details):** A score reflecting the execution efficiency of a correct query compared to its ground truth counterpart. Gives further insights on correctness measure. Higher is better.
- **Bird Standard F1 (Details):** A cell-level F1 score providing insight into the correctness of individual data points within the result sets.
- **Percent Row Count Match (Details):** Indicates if generated queries return the correct number of rows. This can help identify structural correctness even if specific values differ, potentially highlighting issues like filter failures.

Results are broken down by difficulty categories (simple, moderate, challenging).

## How It Works

The evaluation process, primarily orchestrated by `combined_eval.py`, involves several stages:

### 1. AI SDK VQL Generation and Initial Data Collection

- **Script:** `ai_sdk_utils.py` (specifically `generate_aisdk_responses_as_dataframe`)
- The process starts by reading an input Excel file containing questions, their corresponding ground truth VQL queries, difficulty levels, and optional evidence.
- For each question, it calls the Denodo AI SDK API to generate a VQL query.
- It also retrieves associated metadata from the AI SDK, such as `sql_execution_time`, `vector_store_search_time`, `llm_time`, and `total_execution_time`.
- The output is a DataFrame that includes the original questions, generated VQL, ground truth VQL, and AI SDK metadata. This DataFrame serves as the input for subsequent evaluation steps.

### 2. Correctness Evaluation

- **Script:** `f1_eval.py`
- This stage focuses on the accuracy of the data returned by the generated VQL queries compared to the ground truth VQL.
- **Result Set Comparison:** Both the generated VQL and the ground truth VQL are executed against the Denodo database (via `db_utils.execute_vql`).

- **F1 Score Calculation (`f1_score`):**

- Compares result sets by converting rows to tuples of normalized string values.
- Calculates True Positives (matching cells/elements), False Positives, and False Negatives.
- Derives precision, recall, and the "Bird Standard F1" score.
- **Percent Overlap (`percent_overlapp`):** Calculates the percentage of common values between the two complete result sets, considering the frequency of each value.
- **Subsetting Percentage (`set_precision`):** Calculated based on row-wise comparisons. For each pair of rows (one from generated, one from ground truth, at the same index), it determines the proportion of elements in the generated row that are present in the ground truth row. The final metric is an average of these row scores.
- **Structural Comparison:** Uses `db_utils.add_query_execution_data` to determine if the generated and ground truth queries produce the same number of rows (`same_row_count`) and columns (`same_column_count`).
- Utilizes multiprocessing for parallel execution of VQL queries to speed up the evaluation.

#### F1 Calculation Example (Cell-Based)

If Ground Truth Expected Result is:
|       | ColA | ColB  | ColC |
|-------|------|-------|------|
| Row1  | John | Doe   | 35   |
| Row2  | Jane | Smith | 28   |

And Generated Result is:
|       | ColX | ColY    |
|-------|------|---------|
| Row1  | John | Doe     |
| Row2  | Jane | Smith   |
| Row3  | Bob  | Johnson |

- True Positives (TP) = 4 (Matching cells: 'John', 'Doe', 'Jane', 'Smith')
- False Positives (FP) = 2 (Extra cells in prediction: 'Bob', 'Johnson')
- False Negatives (FN) = 2 (Missing cells from ground truth: '35', '28')
- Precision = TP / (TP + FP) = 4 / (4 + 2) = 0.667
- Recall = TP / (TP + FN) = 4 / (4 + 2) = 0.667
- F1 Score = 2 * (Precision * Recall) / (Precision + Recall) = 0.667

### 3. Efficiency Evaluation (VES Calculation)

- **Script:** `ves_eval.py`
- This stage assesses the execution efficiency of the generated VQL queries.
- **Result Verification (`compare_vql_execution`):** First, it confirms that the generated VQL query produces results identical to the ground truth VQL query. If the results do not match, the VES score is typically 0.
- **Iterative Execution (`iterated_execute_vql`):** If the results match, both queries are executed multiple times (controlled by `--iterate-num`).
- **Time Ratio and VES Score:** The average execution times are compared, and this ratio is translated into a Valid Efficiency Score (VES). Outlier execution times can be removed before calculating the average.
- Handles timeouts and execution errors, assigning a low or zero score in such cases.
- Also uses multiprocessing for parallel VQL execution.

### 4. Combined Evaluation and Reporting

- **Script:** `combined_eval.py`
- This script orchestrates the entire workflow:
    1.  Invokes the AI SDK VQL generation step.
    2.  Passes the resulting DataFrame to the F1 evaluation logic.
    3.  Passes the DataFrame to the VES evaluation logic.
    4.  Merges all collected metrics (AI SDK metadata, F1 scores, VES scores, structural comparisons) into a single, comprehensive DataFrame.
    5.  Generates the final Excel report with "Summary" and "Details" sheets, including visualizations.

## Installing the Environment

### Requirements
- Python 3.12+
- Denodo Platform 9.0 or higher, with Data Catalog enabled and accessible.
- Denodo AI SDK deployed and accessible via its API.
- Dependencies:

```
pandas
argparse
tqdm
func_timeout
requests
openpyxl
xlsxwriter
dotenv
```

### Steps

1.  **Navigate to the project root directory:**
```bash
cd path/to/aisdk-eval-main
```
2.  **Create a Python virtual environment:**

```bash
uv venv
```
3.  **Activate the virtual environment:**
- Windows:

    ```bash
    .venv\Scripts\activate
    ```

- Linux/macOS:
    ```bash
    source .venv/bin/activate
    ```

4.  **Install the required dependencies:**

```bash
uv pip install -r requirements.txt
```

5. **Configure Data Catalog connection:**
    Create or verify the `eval/project_config.env` file with your Denodo Data Catalog details (used by `db_utils.py`):

    ```bash
    # filepath: eval/project_config.env
    DATA_CATALOG_URL=http://your-denodo-server:9090/denodo-data-catalog
    DATA_CATALOG_EXECUTION_URL=http://your-denodo-server:9090/denodo-data-catalog/public/api/askaquestion/execute
    DATA_CATALOG_SERVER_ID=1 # Your Denodo server ID in Data Catalog
    DATA_CATALOG_VERIFY_SSL=0 # 1 for true, 0 for false
    ```

    The `db_utils.initialize_data_catalog` function is called by `combined_eval.py` using the VDP credentials provided as command-line arguments, which are then used for Data Catalog VQL execution.

## Usage

The primary way to run the evaluation is using `combined_eval.py`, which orchestrates all steps. Individual scripts (`ai_sdk_utils.py`, `f1_eval.py`, `ves_eval.py`) can also be run for specific parts of the evaluation if needed. All scripts are typically run from within the `eval` directory.

### `combined_eval.py` (Recommended)

Runs the entire pipeline: AI SDK generation, F1 evaluation, VES evaluation, and merges results.

**Command:**

```bash
python combined_eval.py --input ../sample_input.xlsx --output ../results/combined_evaluation.xlsx [options]
```

**Parameters:**

- `--input`/`-i`: (Required) Input Excel file with source data (questions, expected VQL, etc.).
- `--output`/`-o`: Output Excel file for final merged results (default: `combined_results.xlsx`).
- `--f1-output`: Intermediate F1 evaluation output file (default: `None`, not saved separately).
- `--ves-output`: Intermediate VES evaluation output file (default: `None`, not saved separately).
- `--output-original`: Intermediate output Excel from AI SDK responses (default: `original_results.xlsx`, not saved separately if `None`).
- `--question-column`: Column name for questions (default: `"Question"`).
- `--expected-column`: Column name for expected VQL/solution (default: `"Solution"`).
- `--difficulty-col`: Column name for difficulty levels (default: `"difficulty"`).
- `--evidence-column`: Column name for evidence/context for questions (default: `None`).
- `--api-url`: AI SDK API endpoint URL (default: `http://127.0.0.1:8008/answerDataQuestion`).
- `--user`: AI SDK API username (default: `admin`).
- `--password`: AI SDK API password (default: `admin`).
- `--max-workers`: Max parallel workers for AI SDK API calls (default: `10`). This also influences the number of CPUs used for parallel VQL execution in F1/VES stages.
- `--iterate-num`: Number of iterations for VES time comparison (default: `10`).
- `--timeout`/`-t`: Query execution timeout in seconds for F1/VES (default: `30.0`).
- `--db-config`/`-d`: Database configuration JSON file (alternative to individual DB parameters).
- `--question-rows`: Limit number of questions to process from the input file (default: all).

1. **Prepare your input Excel file and config file** (e.g., `sample_input.xlsx` in the project root) with columns like "Question", "Solution" (ground truth VQL), and "difficulty". Also ensure the connection profile in project_config allows you to connect to the Data Catalog

2. **Navigate to the `eval` directory:**

    ```bash
    cd path/to/aisdk-eval-main/eval
    ```

3. **Activate the virtual environment:**
    - Windows:

        ```bash
        ..\venv\Scripts\activate
        ```

    - Linux/macOS:

        ```bash
        source ../venv/bin/activate
        ```

4. **Run the combined evaluation using**
`combined_eval.py`:
    ```bash
    python combined_eval.py ^
      --input "../sample_input.xlsx" ^
      --output "../results/evaluation_results.xlsx" ^
      --question-column "Question" ^
      --expected-column "Solution" ^
      --difficulty-col "difficulty" ^
      --api-url "http://your-aisdk-server:8008/answerDataQuestion" ^
      --user "your_api_user" ^
      --password "your_api_password" ^
      --question-rows 50
    ```
    *(Note: `^` is the line continuation character for Windows Command Prompt. Use `\` for Linux/macOS shells.)*

This will generate an Excel file (e.g., `../results/evaluation_results.xlsx`) with "Summary" and "Details" sheets containing the comprehensive evaluation.



